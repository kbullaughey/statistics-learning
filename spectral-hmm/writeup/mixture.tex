\documentclass[10pt]{article}
\usepackage{cite}
\usepackage{mathrsfs}
\usepackage[labelfont=bf,labelsep=period,justification=raggedright,width=1.15\textwidth]{caption}
\usepackage{subfig}
%\usepackage{sidecap}
\usepackage[subfigure]{tocloft} 
\usepackage{listings} 
%\bibliographystyle{plos2009}
\usepackage[pdftex]{graphicx}
\usepackage{color}
%\makeatletter
%\renewcommand{\@biblabel}[1]{\quad#1.}
%\makeatother
\setlength{\topmargin}{0.0cm}
\setlength{\textheight}{21cm}
\setlength{\textwidth}{16cm}
\setlength{\oddsidemargin}{0.5cm}
\setlength{\evensidemargin}{0.5cm}
\date{}

\usepackage[table]{xcolor}
\definecolor{tableShade}{HTML}{F1F5FA} %iTunes

\setlength{\parskip}{8pt}  
\usepackage[colorlinks]{hyperref}
\hypersetup{linkcolor=black,citecolor=black}
\usepackage{float}
\renewcommand{\tablename}{\textbf{Table}}
\floatstyle{plain}

\clubpenalty=10000
\widowpenalty=10000

\begin{document}
\begin{center}
{\Large
\textbf{Trying out spectral approach for inferring discrete mixtures}
}\\
\vspace{0.3cm}
\large{\textbf{2012 April 23}}

Kevin Bullaughey\\
kbullaughey@gmail.com
\end{center}

\setlength{\parskip}{8pt}
\setlength{\parindent}{1cm}

\section{Description of toy data}

\noindent Topics: $k=3$

\noindent Words: $d=10$

\noindent Documents: 500, each of length 40 words

I generated conditional word probabilities for each word and topic such that I'd have some hope of distinguishing topics. Each topic has roughly a zipfs distribution (albeit for only 10 words) that is subsequently perturbed by enriching for some "topic-specific" words. I perturb the frequencies by adding various vectors of normal quantiles with different means to bump up a few frequencies:

\begin{verbatim}
M <- sapply(c(1,5,9), function(m)
  make.prob.distr(dnorm(1:d, mean=m, sd=1.3)+0.6/(1:d)))
\end{verbatim}

The resulting $M$ matrix is as follows:

\begin{displaymath}
M_{ij} = \mathrm{Prob}(x=e_i|\mathrm{topic}=j)
\end{displaymath}

\begin{displaymath}
\mathbf{M} = 
\left( \begin{array}{ccc} 
0.3762 & 0.2186 & 0.2273 \\
0.2191 & 0.1166 & 0.1137 \\
0.1219 & 0.1066 & 0.0758 \\
0.0711 & 0.1372 & 0.0569 \\
0.0509 & 0.1548 & 0.0465 \\
0.0416 & 0.1191 & 0.0460 \\
0.0356 & 0.0652 & 0.0681 \\
0.0311 & 0.0350 & 0.1149 \\
0.0277 & 0.0252 & 0.1415 \\
0.0249 & 0.0218 & 0.1092
\end{array} \right)
\end{displaymath}

\section{Estimation of $\hat{M}$}

Following the suggestion in the paper, I choose $\vec{\eta} := \hat{U}\vec{\theta}$ where $\vec{\theta}$ is drawn from $\mathcal{S}^{k-1}$. 
To assess the quality of the estimate, I assigned each document to a topic by computing the log likelihood of each document relative to each topic and choosing the best topic: $\mathrm{argmax}_i \mathrm{Prob}(x_{1:n} | \vec{\mu}_i )$ where $\vec{\mu}_i$ is from $\hat{M}$. Given $\hat{M}$ might have its columns permuted relative to $M$, I remapped the topic assignments according to the permutation that resulted in the highest accuracy, where the accuracy is fraction of correct assignments. 

I repeated this for 100 random $\vec{\theta}$. Figure \ref{accuracies}a shows the distribution of accuracies for these 100 $\vec{\eta}$. As can be seen, for some $\vec{\eta}$ the accuracy is very bad.
Another way to assess the quality of the estimates is to use the KL divergence of $M \mathrm{diag}(\vec{w})$ and $\hat{M} \mathrm{diag}(\vec{w})$ (Figure \ref{accuracies}b).
I can take the median probability for each $M_{ij}$ over the 100 $\vec{\eta}$, and re-normalize the conditional probabilities so the columns sum to one, giving me this $\hat{M}$:

\begin{displaymath}
\mathbf{M} = 
\left( \begin{array}{ccc} 
0.2290 & 0.2857 & 0.2228 \\
0.1173 & 0.1563 & 0.1097 \\
0.0973 & 0.1024 & 0.0720 \\
0.1359 & 0.0905 & 0.0593 \\
0.1434 & 0.0719 & 0.0492 \\
0.1170 & 0.0553 & 0.0466 \\
0.0652 & 0.0785 & 0.0700 \\
0.0413 & 0.0646 & 0.1123 \\
0.0314 & 0.0543 & 0.1438 \\
0.0221 & 0.0405 & 0.1142
\end{array} \right)
\end{displaymath}

Which has a KL-divergence of 0.6.

\begin{figure}[btp]
\begin{center}
\resizebox{1.0\columnwidth}{!}{\includegraphics{accuracies.pdf}}
\caption{}
\label{accuracies}
\end{center}
\end{figure}

\end{document}
